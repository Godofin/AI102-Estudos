# Criar aplicativos habilitados para fala com os serviços de IA do Azure

## Introdução

- O Fala de IA do Azure fornece APIs que você pode utilizar para criar aplicativos habilitados para fala. Isso inclui:

- Conversão de fala em texto: uma API que habilita o reconhecimento de fala no qual seu aplicativo pode aceitar uma entrada falada.
- Conversão de texto em fala: uma API que habilita a sintetização de voz em que o aplicativo pode fornecer a saída de fala.
- Tradução de Fala: uma API que você pode usar para traduzir a entrada falada para vários idiomas.
- Reconhecimento do Locutor: uma API que permite que o aplicativo reconheça cada falante com base na voz.
- Reconhecimento de intenção: uma API que utiliza a compreensão da linguagem coloquial para determinar o significado semântico da entrada falada.

## Provisionar um recurso do Azure para fala

- Antes de poder utilizar o Fala de IA do Azure, você precisa criar um recurso do Fala de IA do Azure na sua assinatura do Azure. Você pode utilizar um recurso dedicado do Fala de IA do Azure ou um recurso de vários serviços dos Serviços de IA do Azure.
- Depois de criar o recurso, você precisará das seguintes informações para utilizá-lo em um aplicativo cliente por meio de um dos SDKs com suporte:
    - A localização na qual o recurso foi implantado (por exemplo, lesteeua)
    - Uma das chaves atribuídas ao seu recurso.

## Usar a API de Conversão de Fala em Texto da IA do Azure

- O serviço de fala do Azure é compatível com o reconhecimento de fala por meios de duas APIs REST
  - A API de conversão de fala em texto, que é principal maneira de fazer reconhecimento em fala
  - A API de conversão de fala em texto para aúdio curto que é otimizada para áudios curtos de até 60 segundos
- Também é possível utilizar a api de conversão de fala em texto para transcrição em lote transcrevendo vários arquivos de aúdio em texto como operação em lote

### Usar o SDK da Fala de IA do Azure

- Use um objeto SpeechConfig para encapsular as informações necessárias para se conectar ao seu recurso da Fala de IA do Azure. Especificamente, sua localização e chave.
- Como alternativa, use um AudioConfig para definir a fonte de entrada para o áudio a ser transcrito. Por padrão, a entrada é o microfone padrão do sistema, mas você também pode especificar um arquivo de áudio.
- Use SpeechConfig e AudioConfig para criar um objeto SpeechRecognizer. Esse objeto é um cliente proxy para a API de conversão de fala em texto.
- Use os métodos do objeto SpeechRecognizer para chamar as funções de API subjacentes. Por exemplo, o método RecognizeOnceAsync() usa o serviço de Fala de IA do Azure para traduzir de modo assíncrono um único enunciado falado.
- Processe a resposta do serviço de Fala de IA do Azure. No caso do método RecognizeOnceAsync(), o resultado é um objeto SpeechRecognitionResult que inclui as seguintes propriedades:
    - Duration
    - OffsetInTicks
    - Propriedades
    - Motivo
    - ResultId
    - Texto
- Se a operação tiver sido bem-sucedida, a propriedade Reason terá o valor enumerado RecognizedSpeech e a propriedade Text conterá a transcrição. Outros valores possíveis para Resultado incluem NoMatch (indicando que o áudio foi analisado com êxito, mas nenhuma fala foi reconhecida) ou Cancelado, indicando que ocorreu um erro (nesse caso, você pode verificar a propriedade CancellationReason na coleção Propriedades para determinar o que deu errado).

## Usar a API de conversão de texto em fala

- A API de conversão de texto em fala, que é a principal maneira de fazer a sintetização de voz.
- A API de Síntese em lote, projetada para operações em lote que convertem grandes volumes de texto em áudio – por exemplo, para gerar um audiolivro com base no texto de origem.

- Use um objeto SpeechConfig para encapsular as informações necessárias para se conectar ao seu recurso da Fala de IA do Azure. Especificamente, sua localização e chave.
- Opcionalmente, use AudioConfig para definir o dispositivo de saída para a fala a ser sintetizada. Por padrão, é o alto-falante do sistema padrão, mas você também pode especificar um arquivo de áudio ou definir explicitamente esse valor como nulo para processar o objeto de fluxo de áudio retornado diretamente.
- Use SpeechConfig e AudioConfig para criar um objeto SpeechSynthesizer. Esse objeto é um cliente proxy para a API de conversão de texto em fala.
- Use os métodos do objeto SpeechSynthesizer para chamar as funções de API subjacentes. Por exemplo, o método SpeakTextAsync() usa o serviço de Fala de IA do Azure para converter texto em áudio falado.
Processe a resposta do serviço de Fala de IA do Azure. No caso do método SpeakTextAsync, o resultado é um objeto SpeechSynthesisResult que contém as seguintes propriedades:
  - AudioData
  - Propriedades
  - Motivo
  - ResultId

- Quando a fala é sintetizada com êxito, a propriedade Reason é definida como a enumeração SynthesizingAudioCompleted e a propriedade AudioData contém o fluxo de áudio (que, dependendo de AudioConfig, pode ter sido enviada automaticamente para um alto-falante ou arquivo).